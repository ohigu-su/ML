###[data 준비]###
##import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer

import warnings
warnings.filterwarnings(action='ignore')

##data 불러오기->데이터의 key를 확인 
breast_cancer = load_breast_cancer()
breast_cancer.keys()

df = pd.DataFrame(data=breast_cancer.data,columns=breast_cancer.feature_names)
df

df = df.iloc[:,:10] #0~9열 가져오기
df["result"]=breast_cancer.target #target설정
df.columns=[col.replace(" ","_") for col in df.columns] #columns이름 수정

df.head(3)

###[dataset 나누기]###
from sklearn.model_selection import train_test_split

#train/test
train,test = train_test_split(df,test_size=0.15,random_state=123) #test dataset로 dataset 15%만 사용

#train을 train/validation으로 
train,val=train_test_split(train,test_size=0.18,random_state=123)

train.shape,val.shape,test.shape

###[기준모델 만들기]###
from sklearn.metrics import accuracy_score

#모델 만들기
majority_class=train["result"].mode()[0] #Series의 형태이므로 index와 함께 표시, 즉 [0]=index
#type([train["result"].mode()[0]])=list

#기준모델의 정확도 계산을 위한 데이터 생성
y_pred = [majority_class]*len(val) #len(validation(=val))=87이므로 1이 87개로 이루어진 list형성

#validation 데이터셋에 대한 정확도 확인
print("최빈 클래스 : ", majority_class)
print("validation 데이터셋 정확도 : ", accuracy_score(val["result"],y_pred))
# accuracy_score(val["result"],y_pred) ; val["result"]와 y_pred가 일치할 확률

###[feature 간 상관성 확인]###
import seaborn as sns
sns.pairplot(data=df)
plt.show()
##[결과1]mean_radius, mean_perimeter, mean_area 사이에 강한 상관관계
##[결과2]mean_compactness, mean_concave_points, mean_concavity 사이에도 강한 상관관계
##[결론] mean_texture, mean_smoothness, mean_symmetry, mean_fractal_dimension과 mean_area, mean_concave_points를 이용하여 모델을 만든다.

# feature/taraget 설정
feature = ["mean_texture", "mean_smoothness", "mean_symmetry", "mean_fractal_dimension", "mean_radius", "mean_compactness"]
target = ["result"]

# train 데이터셋
X_train = train[feature]
y_train = train[target]

# validation 데이터셋
X_val = val[feature]
y_val = val[target]

# test 데이터셋
X_test = test[feature]
y_test = test[target]

# 확인
print("feature Matrix: ", X_train.shape, X_val.shape, X_test.shape)
print("target vector: ", y_train.shape, y_val.shape, y_test.shape)

###[스케일 조정]###
from sklearn.preprocessing import StandardScaler

#스케일러 생성
scaler = StandardScaler()

#스케일 조정
X_train_scaled = scaler.fit_transform(X_train)
X_val_scaled = scaler.fit_transform(X_val)
X_test_scaled = scaler.fit_transform(X_test)

X_train_scaled.T[0].mean(),X_train_scaled.std()

###[Logistic Regression 모델 만들기]###
from sklearn.linear_model import LogisticRegression

#모델 생성, 학습
logistic=LogisticRegression()
logistic.fit(X_train_scaled,y_train)

#결과
print("validation 데이터셋 정확도")
logistic.score(X_val_scaled,y_val)

###[최종 모델 만들기]###
# train 데이터셋과 validation 데이터셋 합치기
X_total = pd.concat([X_train, X_test])
y_total = pd.concat([y_train, y_test])

# feature/taraget 설정
feature = ["mean_texture", "mean_smoothness", "mean_symmetry", "mean_fractal_dimension", "mean_radius", "mean_concave_points"]
target = "result"

# 스케일러 생성
scaler = StandardScaler()

# 스케일 조정
X_total_sclaed = scaler.fit_transform(X_total)
X_test_scaled = scaler.transform(X_test)

# 모델 생성 및 학습 시키기
model = LogisticRegression()
model.fit(X_total_sclaed, y_total)

# 결과 확인
print("test 데이터셋 정확도")
model.score(X_test_scaled, y_test)

